{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4dc8bf",
   "metadata": {},
   "source": [
    "**Two types**\n",
    "\n",
    "Absolute: *For each position in input sequence, a unique embedding is added to the token's embedding to convey its exact loaction*\n",
    "\n",
    "Input embedding + positional embedding = token embedding\n",
    "\n",
    "Relative:*The emphasis is on the relative position or the distance between tokens. the model learnsthe relationship in terms of \"how far apart\" rather than at which exact position*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "611d9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b736f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/anshk/OneDrive/Desktop/LLM/Datasets/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff1fccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx] , self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd64fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c10dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "704806a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride = max_length,shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs , targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97ef6d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "token IDs shape: torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"token IDs:\\n\", inputs)\n",
    "print(\"token IDs shape:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a68cb98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token embedding:\n",
      " tensor([[[ 0.1502,  1.7766, -0.4974,  ...,  1.3028, -0.5298,  0.1891],\n",
      "         [-0.7940,  1.5534, -0.9948,  ...,  0.3806, -0.6563,  0.6941],\n",
      "         [-0.3718, -1.0039,  1.5890,  ..., -1.7633, -0.6094,  0.3986],\n",
      "         [ 0.6215, -0.8419, -0.6899,  ...,  0.7351,  0.4315, -0.2839]],\n",
      "\n",
      "        [[ 0.2301, -0.4256, -1.0295,  ..., -0.5378,  2.4979,  0.0975],\n",
      "         [-1.2164,  1.0402,  1.6129,  ..., -1.6432, -0.7806,  0.5704],\n",
      "         [ 0.8155, -0.3002,  2.1801,  ...,  0.8985,  0.0634,  0.5205],\n",
      "         [ 0.1896, -1.5521,  0.6078,  ..., -1.1363,  0.1531,  0.9384]],\n",
      "\n",
      "        [[ 0.1036,  1.8998, -0.4487,  ...,  0.5173, -0.0386, -1.2152],\n",
      "         [ 1.2996, -0.0690,  0.2348,  ...,  0.7924, -0.7509,  1.2181],\n",
      "         [-0.1985, -0.4691, -1.4262,  ...,  0.9589,  0.1182,  0.8242],\n",
      "         [-0.6293, -0.2836,  0.7863,  ...,  0.5548,  0.1321, -1.3405]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2612,  0.3330,  0.7294,  ..., -1.0338, -0.3780, -0.8606],\n",
      "         [ 2.0468, -0.4125,  2.7718,  ...,  0.0425, -2.5070,  0.5615],\n",
      "         [ 1.0130, -0.5422,  0.9967,  ..., -0.2576, -0.5694, -0.5465],\n",
      "         [ 0.6469,  0.4452, -0.1851,  ...,  0.9790, -0.2805, -0.7406]],\n",
      "\n",
      "        [[ 0.3231, -0.9390,  2.6172,  ..., -1.0433, -0.0501, -0.6572],\n",
      "         [-1.4733,  0.2059,  1.0090,  ..., -1.0221, -0.3600, -0.1838],\n",
      "         [-1.7773, -0.4593, -0.2752,  ..., -0.4843,  1.1932,  0.5855],\n",
      "         [ 1.3217, -1.2788, -0.2653,  ..., -0.8462, -1.2567, -0.6032]],\n",
      "\n",
      "        [[-1.7773, -0.4593, -0.2752,  ..., -0.4843,  1.1932,  0.5855],\n",
      "         [-0.1508,  1.6796, -0.2455,  ..., -0.0932, -0.1079, -0.9843],\n",
      "         [ 1.0292, -0.5627,  0.2995,  ..., -0.7465,  1.4360, -0.1164],\n",
      "         [ 0.4637,  0.7048, -0.6248,  ..., -0.3107,  1.4434,  1.6119]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "token embedding shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embedding = token_embedding_layer(inputs)\n",
    "print(\"token embedding:\\n\", token_embedding)\n",
    "print(\"token embedding shape:\", token_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0d6147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "555adae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional embedding:\n",
      " tensor([[-0.4888,  0.2885,  0.5037,  ...,  0.3666,  0.7809,  0.9031],\n",
      "        [-0.0792,  0.5920,  0.8640,  ..., -0.3289,  0.1341,  1.2895],\n",
      "        [ 0.3288, -0.8666, -0.8885,  ..., -0.4288, -1.4908,  0.4945],\n",
      "        [-0.3368, -1.1718,  0.0603,  ..., -0.8182,  0.5480, -0.3375]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "positional embedding shape: torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embedding = pos_embedding_layer(torch.arange(max_length))\n",
    "print(\"positional embedding:\\n\", pos_embedding)\n",
    "print(\"positional embedding shape:\", pos_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ced6944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input embedding:\n",
      " tensor([[[-0.3385,  2.0650,  0.0063,  ...,  1.6695,  0.2511,  1.0923],\n",
      "         [-0.8732,  2.1453, -0.1307,  ...,  0.0517, -0.5223,  1.9837],\n",
      "         [-0.0429, -1.8705,  0.7005,  ..., -2.1921, -2.1001,  0.8931],\n",
      "         [ 0.2846, -2.0137, -0.6295,  ..., -0.0831,  0.9795, -0.6215]],\n",
      "\n",
      "        [[-0.2586, -0.1371, -0.5258,  ..., -0.1712,  3.2788,  1.0006],\n",
      "         [-1.2956,  1.6321,  2.4769,  ..., -1.9720, -0.6465,  1.8600],\n",
      "         [ 1.1443, -1.1667,  1.2916,  ...,  0.4697, -1.4274,  1.0150],\n",
      "         [-0.1472, -2.7239,  0.6681,  ..., -1.9545,  0.7011,  0.6009]],\n",
      "\n",
      "        [[-0.3852,  2.1883,  0.0550,  ...,  0.8839,  0.7423, -0.3121],\n",
      "         [ 1.2204,  0.5229,  1.0988,  ...,  0.4636, -0.6168,  2.5076],\n",
      "         [ 0.1304, -1.3357, -2.3148,  ...,  0.5301, -1.3726,  1.3188],\n",
      "         [-0.9661, -1.4554,  0.8467,  ..., -0.2634,  0.6801, -1.6780]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7499,  0.6215,  1.2331,  ..., -0.6672,  0.4029,  0.0425],\n",
      "         [ 1.9675,  0.1794,  3.6358,  ..., -0.2863, -2.3730,  1.8511],\n",
      "         [ 1.3418, -1.4088,  0.1082,  ..., -0.6863, -2.0602, -0.0520],\n",
      "         [ 0.3101, -0.7266, -0.1248,  ...,  0.1608,  0.2675, -1.0781]],\n",
      "\n",
      "        [[-0.1657, -0.6505,  3.1209,  ..., -0.6767,  0.7308,  0.2459],\n",
      "         [-1.5525,  0.7978,  1.8730,  ..., -1.3509, -0.2259,  1.1057],\n",
      "         [-1.4485, -1.3259, -1.1637,  ..., -0.9131, -0.2976,  1.0800],\n",
      "         [ 0.9849, -2.4506, -0.2050,  ..., -1.6644, -0.7087, -0.9407]],\n",
      "\n",
      "        [[-2.2661, -0.1708,  0.2286,  ..., -0.1177,  1.9740,  1.4886],\n",
      "         [-0.2300,  2.2716,  0.6185,  ..., -0.4221,  0.0262,  0.3052],\n",
      "         [ 1.3580, -1.4293, -0.5891,  ..., -1.1753, -0.0548,  0.3781],\n",
      "         [ 0.1268, -0.4670, -0.5645,  ..., -1.1289,  1.9914,  1.2744]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "input embedding shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embedding = token_embedding + pos_embedding\n",
    "print(\"input embedding:\\n\", input_embedding)\n",
    "print(\"input embedding shape:\", input_embedding.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
