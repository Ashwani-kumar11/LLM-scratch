{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4dc8bf",
   "metadata": {},
   "source": [
    "**Two types**\n",
    "\n",
    "Absolute: *For each position in input sequence, a unique embedding is added to the token's embedding to convey its exact loaction*\n",
    "\n",
    "Input embedding + positional embedding = token embedding\n",
    "\n",
    "Relative:*The emphasis is on the relative position or the distance between tokens. the model learnsthe relationship in terms of \"how far apart\" rather than at which exact position*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611d9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b736f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/anshk/OneDrive/Desktop/LLM/Datasets/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1fccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx] , self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd64fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c10dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704806a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride = max_length,shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs , targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97ef6d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "token IDs shape: torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"token IDs:\\n\", inputs)\n",
    "print(\"token IDs shape:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68cb98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token embedding:\n",
      " tensor([[[-0.2440,  0.7625, -0.5820,  ..., -1.2853,  2.0107,  0.6343],\n",
      "         [ 0.0252,  1.7414,  0.5879,  ...,  0.5811, -0.4826,  0.4717],\n",
      "         [ 0.2577, -0.3697,  0.6461,  ...,  1.3514, -1.3790,  0.3190],\n",
      "         [-0.4630, -0.2103, -1.0037,  ...,  0.2804, -0.4760, -1.1063]],\n",
      "\n",
      "        [[-0.1983, -1.0045,  0.4638,  ..., -0.5927,  0.4291,  1.3377],\n",
      "         [ 0.8566,  2.3831, -1.4070,  ..., -0.5381,  0.2112,  0.6770],\n",
      "         [ 1.6264, -0.9534,  0.2101,  ...,  0.6573, -1.0363,  1.1449],\n",
      "         [-1.6295, -1.1488, -0.2221,  ..., -0.3448, -1.9262, -1.1214]],\n",
      "\n",
      "        [[-0.1428, -1.4351,  0.3176,  ...,  0.6982,  1.0452,  0.6206],\n",
      "         [ 0.3215,  0.6797, -1.4317,  ...,  0.5068, -1.3675, -0.0350],\n",
      "         [ 1.5025, -0.4257, -0.4658,  ...,  0.6983, -0.4191, -0.6936],\n",
      "         [-1.3856,  0.0792,  0.6901,  ...,  1.8047, -0.4890, -0.0627]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.5745, -1.5984,  0.7439,  ..., -0.5898,  0.1458,  0.3496],\n",
      "         [-0.5216, -0.7947, -0.5601,  ...,  0.5897, -0.6068,  0.6596],\n",
      "         [-1.9278, -0.2177, -0.4498,  ...,  0.2594,  1.0724,  0.7402],\n",
      "         [ 0.6812,  1.0932, -0.8114,  ...,  0.9508,  1.3547,  1.8754]],\n",
      "\n",
      "        [[ 0.2811, -1.2459,  1.6342,  ...,  0.7284,  1.1829,  0.1397],\n",
      "         [ 0.6368, -2.0404, -1.7409,  ..., -0.2884, -1.4339,  0.0307],\n",
      "         [ 1.8154, -0.0252, -0.5319,  ...,  0.1560,  0.6117,  0.8129],\n",
      "         [ 1.3994, -1.4745, -0.9238,  ...,  1.0422, -0.8841,  0.6007]],\n",
      "\n",
      "        [[ 1.8154, -0.0252, -0.5319,  ...,  0.1560,  0.6117,  0.8129],\n",
      "         [ 1.6580,  0.7168,  0.9116,  ...,  1.3743,  1.4986,  0.5888],\n",
      "         [-0.8894, -1.6254,  1.7168,  ..., -1.2412,  0.6637,  0.7140],\n",
      "         [ 1.0952,  0.2662,  0.4348,  ..., -0.7047, -0.8360, -0.9653]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "token embedding shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embedding = token_embedding_layer(inputs)\n",
    "print(\"token embedding:\\n\", token_embedding)\n",
    "print(\"token embedding shape:\", token_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0d6147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "555adae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional embedding:\n",
      " tensor([[ 0.5830, -1.0771,  0.3607,  ...,  0.8952, -0.0574,  1.9709],\n",
      "        [-0.5046, -0.0031, -2.0193,  ...,  1.1257,  0.4667, -0.2845],\n",
      "        [-0.5482, -0.4945,  1.1000,  ...,  0.2527,  0.4895, -0.5412],\n",
      "        [-0.4151, -0.4484, -0.5756,  ...,  1.0777, -0.0704,  0.2744]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "positional embedding shape: torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embedding = pos_embedding_layer(torch.arange(max_length))\n",
    "print(\"positional embedding:\\n\", pos_embedding)\n",
    "print(\"positional embedding shape:\", pos_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ced6944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input embedding:\n",
      " tensor([[[ 0.3389, -0.3146, -0.2212,  ..., -0.3901,  1.9533,  2.6052],\n",
      "         [-0.4794,  1.7383, -1.4314,  ...,  1.7068, -0.0160,  0.1873],\n",
      "         [-0.2905, -0.8642,  1.7462,  ...,  1.6042, -0.8895, -0.2221],\n",
      "         [-0.8781, -0.6587, -1.5793,  ...,  1.3581, -0.5464, -0.8319]],\n",
      "\n",
      "        [[ 0.3847, -2.0816,  0.8246,  ...,  0.3025,  0.3716,  3.3086],\n",
      "         [ 0.3520,  2.3800, -3.4263,  ...,  0.5876,  0.6778,  0.3925],\n",
      "         [ 1.0783, -1.4479,  1.3102,  ...,  0.9100, -0.5468,  0.6038],\n",
      "         [-2.0446, -1.5972, -0.7977,  ...,  0.7329, -1.9965, -0.8471]],\n",
      "\n",
      "        [[ 0.4402, -2.5122,  0.6783,  ...,  1.5934,  0.9878,  2.5915],\n",
      "         [-0.1831,  0.6766, -3.4510,  ...,  1.6325, -0.9008, -0.3195],\n",
      "         [ 0.9544, -0.9202,  0.6342,  ...,  0.9510,  0.0704, -1.2348],\n",
      "         [-1.8007, -0.3692,  0.1145,  ...,  2.8824, -0.5594,  0.2116]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.1574, -2.6755,  1.1046,  ...,  0.3053,  0.0884,  2.3205],\n",
      "         [-1.0262, -0.7978, -2.5794,  ...,  1.7154, -0.1401,  0.3752],\n",
      "         [-2.4760, -0.7122,  0.6502,  ...,  0.5122,  1.5619,  0.1990],\n",
      "         [ 0.2661,  0.6448, -1.3870,  ...,  2.0285,  1.2843,  2.1497]],\n",
      "\n",
      "        [[ 0.8641, -2.3230,  1.9949,  ...,  1.6235,  1.1255,  2.1106],\n",
      "         [ 0.1322, -2.0434, -3.7602,  ...,  0.8373, -0.9672, -0.2538],\n",
      "         [ 1.2673, -0.5197,  0.5681,  ...,  0.4087,  1.1012,  0.2718],\n",
      "         [ 0.9843, -1.9229, -1.4994,  ...,  2.1199, -0.9545,  0.8751]],\n",
      "\n",
      "        [[ 2.3984, -1.1023, -0.1712,  ...,  1.0512,  0.5543,  2.7839],\n",
      "         [ 1.1534,  0.7137, -1.1077,  ...,  2.5000,  1.9653,  0.3043],\n",
      "         [-1.4376, -2.1198,  2.8169,  ..., -0.9885,  1.1532,  0.1728],\n",
      "         [ 0.6801, -0.1822, -0.1408,  ...,  0.3730, -0.9063, -0.6909]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "input embedding shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embedding = token_embedding + pos_embedding\n",
    "print(\"input embedding:\\n\", input_embedding)\n",
    "print(\"input embedding shape:\", input_embedding.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
