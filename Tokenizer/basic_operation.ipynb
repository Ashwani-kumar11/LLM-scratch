{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea5eb77",
   "metadata": {},
   "source": [
    "REading the file and creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6ed3ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the file: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open (\"C:/Users/anshk/OneDrive/Desktop/LLM/Datasets/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Total number of characters in the file:\", len(text))\n",
    "print(text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ecf379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the file: 7431\n",
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius--though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough--so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that,', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory,', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting,', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow,', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a', ' ', 'villa', ' ', 'on', ' ', 'the', ' ', 'Riviera.', ' ', '(Though', ' ', 'I']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(\"Total number of tokens in the file:\", len(result))\n",
    "print(result[:99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61114da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the file: 8313\n",
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius--though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough--so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that', ',', '', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory', ',', '', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting', ',', '', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow', ',', '', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a', ' ', 'villa', ' ', 'on']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(\"Total number of tokens in the file:\", len(result))\n",
    "print(result[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c02e983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the file: 4092\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius--though', 'a', 'good', 'fellow', 'enough--so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"The', 'height', 'of', 'his', 'glory\"--that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing--his', 'last', 'Chicago', 'sitter--deploring', 'his', 'unaccountable', 'abdication', '.', '\"Of', 'course', \"it's\", 'going', 'to', 'send', 'the']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(\"Total number of tokens in the file:\", len(result))\n",
    "print(result[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "684eea74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the file: 9235\n",
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '--', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that', ',', '', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory', ',', '', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting', ',', '', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow', ',', '', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a']\n",
      "Total number of tokens in the file: 4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# print(text[:99])\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "print(\"Total number of tokens in the file:\", len(result))\n",
    "print(result[:99])\n",
    "result = [item for item in result if item.strip()]\n",
    "print(\"Total number of tokens in the file:\", len(result))\n",
    "print(result[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44ce8079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the file: 4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "print(\"Total number of tokens in the file:\", len(preprocessed))\n",
    "print(preprocessed[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b51a9f",
   "metadata": {},
   "source": [
    "Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82be433b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique tokens in the file: 1130\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(set(preprocessed))\n",
    "\n",
    "print(\"Total number of unique tokens in the file:\", len((tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da62527",
   "metadata": {},
   "source": [
    "Creating vocabulary: \n",
    "\n",
    "vocabulary is Dictionary where, each unique token is mapped to unique tokenID in the lexicographically order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f6db55d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:i for i, token in enumerate(tokens)}\n",
    "print(len(vocab))\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>=25:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec052e",
   "metadata": {},
   "source": [
    "Implement a complete tokenizer class\n",
    "\n",
    "The class has a encode method to map tokens into unique integres\n",
    "\n",
    "Decode method to convert integers back to tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b97dd",
   "metadata": {},
   "source": [
    "Step 1: Store the vovabulary as a class attribute for access in the encode and decode method\n",
    "\n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original tokens\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6072be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the file: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# with open (\"C:/Users/anshk/OneDrive/Desktop/LLM/Datasets/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# print(\"Total number of characters in the file:\", len(text))\n",
    "# print(text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f778dc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "token = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "token = [item for item in token if item.strip()]\n",
    "tokens = sorted(set(token))\n",
    "vocab = {token:i for i, token in enumerate(tokens)}\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d7ccf31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[item] for item in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[item] for item in ids])\n",
    "        #replace spaces before the specified punctuation\n",
    "        text = re.sub(r'([,.:;?_!\"()\\'])\\s', r'\\1', text)\n",
    "        # text = re.sub(r'([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "abfc8c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 44, 149, 1003, 57, 38, 818, 115, 256, 486, 6, 1002, 115, 500, 435, 392, 6, 908, 585, 1077, 709, 508, 961, 1016, 663, 1016, 535, 987, 5, 568, 988, 538, 722, 549, 496, 5, 533, 514, 370, 549, 748, 5, 661, 115, 841, 1102, 5, 157, 397, 547, 568, 115, 1066, 727, 988, 84, 7, 3, 99, 53, 818, 1003, 585, 1120, 530, 208, 85, 734, 34, 7, 4, 1, 93, 538, 722, 549, 496, 1, 6, 987, 1077, 1089, 988, 1112, 242, 585, 7, 53, 244, 535, 67, 7, 37, 100, 6, 549, 602, 25, 897]\n",
      "4690\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "# text = \"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids[:99])\n",
    "print(len(ids))\n",
    "# print(max(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "56e91245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius -- though a good fellow enough -- so it was\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(ids)\n",
    "print(text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc1a718",
   "metadata": {},
   "source": [
    "What if the word that is not present in the vocabulary is encoded using the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6761414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text = \"What is this Henery?\"\n",
    "# ids = tokenizer.encode(text)\n",
    "# print(ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16862a5f",
   "metadata": {},
   "source": [
    "**Henery was not present in the vocab, so it was not encoded**\n",
    "\n",
    "Thus while training LLMs it is necessary to have a large vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4cab70",
   "metadata": {},
   "source": [
    "**Adding Special context tokens**\n",
    "\n",
    "Add <|unk|> for unknown token, <|endoftext|> for indicating the end of the document\n",
    "\n",
    "--When we are working with multiple text sources as the input then we add <|endoftext|> token to identify the end of one source\n",
    "\n",
    "--This leads to more effective processing and understanding by the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "30428d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open (\"C:/Users/anshk/OneDrive/Desktop/LLM/Datasets/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# print(\"Total number of characters in the file:\", len(text))\n",
    "# print(text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0a484361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique tokens in the file: 7\n"
     ]
    }
   ],
   "source": [
    "token2 = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "token2 = [item for item in token2 if item.strip()]\n",
    "# tokens2 = sorted(set(token2))\n",
    "all_tokens = sorted(list(set(token2)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab2 = {token:i for i, token in enumerate(all_tokens)}\n",
    "print(\"Total number of unique tokens in the file:\", len(vocab2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1b55a942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab2.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3b8a7",
   "metadata": {},
   "source": [
    "**SimpleTokenizer Version 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a6badc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        processed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        processed = [item.strip() for item in processed if item.strip()]\n",
    "        processed = [\n",
    "            item if item in self.str_to_int else \"<|unk|>\" for items in processed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in processed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[item] for item in ids])\n",
    "        #replace spaces before the specified punctuation\n",
    "        text = re.sub(r'([,.:;?_!\"()\\'])\\s', r'\\1', text)\n",
    "        # text = re.sub(r'([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b4c73821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, younger do you like tea? <|endoftext|> In the sunlit room, the shadows danced.\n"
     ]
    }
   ],
   "source": [
    "tokenizer2 = SimpleTokenizerV2(vocab2)\n",
    "text1 = \"Hello, younger do you like tea?\"\n",
    "text2 = \"In the sunlit room, the shadows danced.\"\n",
    "\n",
    "text3 = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "42b0e798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 1131, 1131, 1131, 1131, 1131, 1131, 1131, 1131, 1131, 1131, 1131, 1131, 1131, 1131, 1131, 1131, 1131]\n"
     ]
    }
   ],
   "source": [
    "ids =tokenizer2.encode(text3)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b86c1158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|>\n"
     ]
    }
   ],
   "source": [
    "text4 = tokenizer2.decode(ids)\n",
    "print(text4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
